# ml-core/service_random.py
from fastapi import FastAPI
import joblib
import numpy as np
from pydantic import BaseModel
import time
import traceback
from typing import Dict, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Orchid Random Forest - FULL")

from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
rf_model = None
label_encoder = None
scaler = None
model_loaded = False
feature_names = [
    'request_length', 'param_count', 'special_char_ratio',
    'url_depth', 'user_agent_length', 'content_length',
    'request_time_seconds', 'status_code',
    'sql_keywords_count', 'html_tag_count', 'path_traversal_count',
    'entropy', 'max_token_length', 'has_equals', 'has_quotes',
    'digit_count', 'letter_count', 'letter_digit_ratio'
]
label_mapping = {
    0: "normal",
    1: "sqli",
    2: "xss",
    3: "lfi",
    4: "rce",
    5: "brute"
}

def load_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π Random Forest"""
    global rf_model, label_encoder, scaler, model_loaded

    try:
        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Random Forest –º–æ–¥–µ–ª–∏...")
        rf_model = joblib.load('models/random_forest_real.joblib')
        logger.info("‚úÖ Random Forest –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Label Encoder...")
        label_encoder = joblib.load('models/label_encoder.joblib')
        logger.info("‚úÖ Label Encoder –∑–∞–≥—Ä—É–∂–µ–Ω")

        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Scaler...")
        scaler = joblib.load('models/scaler.joblib')
        logger.info("‚úÖ Scaler –∑–∞–≥—Ä—É–∂–µ–Ω")

        # –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏
        test_features = np.random.randn(1, 18)
        test_scaled = scaler.transform(test_features)
        prediction = rf_model.predict(test_scaled)
        logger.info(f"‚úÖ –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–µ–Ω, prediction: {prediction}")

        model_loaded = True
        logger.info(f"‚úÖ Random Forest –∑–∞–≥—Ä—É–∂–µ–Ω. –ö–ª–∞—Å—Å—ã: {label_encoder.classes_}")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        logger.error(traceback.format_exc())
        model_loaded = False

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
load_models()

class PredictionRequest(BaseModel):
    features: Dict[str, Any]
    metadata: Dict[str, Any]

@app.get("/health")
async def health():
    return {
        "status": "healthy" if model_loaded else "unhealthy",
        "service": "Random Forest (FULL)",
        "timestamp": time.time(),
        "model_loaded": model_loaded,
        "model_type": "Random Forest",
        "n_classes": len(label_encoder.classes_) if label_encoder else 0,
        "version": "2.0-full"
    }

@app.get("/model-info")
async def model_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    if not model_loaded:
        return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}

    return {
        "model_type": "Random Forest",
        "n_estimators": rf_model.n_estimators if hasattr(rf_model, 'n_estimators') else "unknown",
        "n_classes": len(label_encoder.classes_) if label_encoder else "unknown",
        "classes": list(label_encoder.classes_) if label_encoder else [],
        "features": feature_names,
        "loaded": model_loaded
    }

@app.post("/predict")
async def predict(request: PredictionRequest):
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∞—Ç–∞–∫–∏"""

    if not model_loaded:
        return {
            "error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞",
            "service": "Random Forest",
            "timestamp": time.time(),
            "model_used": False
        }

    try:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏
        features_list = []
        for feature in feature_names:
            if feature in request.features:
                value = request.features[feature]
                try:
                    features_list.append(float(value))
                except:
                    features_list.append(0.0)
            else:
                # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                if feature == 'status_code':
                    features_list.append(200.0)
                elif feature == 'request_length':
                    features_list.append(500.0)
                elif feature == 'param_count':
                    features_list.append(3.0)
                elif feature == 'special_char_ratio':
                    features_list.append(0.05)
                elif feature == 'url_depth':
                    features_list.append(2.0)
                elif feature == 'user_agent_length':
                    features_list.append(120.0)
                elif feature == 'content_length':
                    features_list.append(800.0)
                elif feature == 'request_time_seconds':
                    features_list.append(0.5)
                else:
                    features_list.append(0.0)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º
        features_array = np.array([features_list])
        features_scaled = scaler.transform(features_array)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ Random Forest
        prediction_encoded = rf_model.predict(features_scaled)[0]
        prediction_proba = rf_model.predict_proba(features_scaled)[0]

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫—É
        try:
            prediction_label = label_encoder.inverse_transform([prediction_encoded])[0]
        except:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º mapping
            prediction_label = label_mapping.get(prediction_encoded, f"class_{prediction_encoded}")

        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = float(prediction_proba[prediction_encoded])

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∞—Ç–∞–∫–æ–π
        is_attack = prediction_label != "normal"

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        top_n = min(3, len(prediction_proba))
        top_indices = np.argsort(prediction_proba)[-top_n:][::-1]
        top_predictions = []

        for idx in top_indices:
            try:
                label = label_encoder.inverse_transform([idx])[0]
            except:
                label = label_mapping.get(idx, f"class_{idx}")
            top_predictions.append({
                "label": label,
                "confidence": float(prediction_proba[idx]),
                "class_id": int(idx)
            })

        response = {
            "prediction": prediction_label,
            "confidence": round(confidence, 3),
            "is_attack": is_attack,
            "service": "Random Forest",
            "timestamp": time.time(),
            "model_used": True,
            "top_predictions": top_predictions,
            "all_probabilities": {
                label_encoder.inverse_transform([i])[0]: float(prediction_proba[i])
                for i in range(len(prediction_proba))
            } if label_encoder else {},
            "raw_features": features_list
        }

        logger.info(f"Random Forest prediction: {response}")
        return response

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
        logger.error(traceback.format_exc())
        return {
            "error": f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}",
            "service": "Random Forest",
            "timestamp": time.time(),
            "model_used": False
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
