# ml-core/service_isolation.py
from fastapi import FastAPI
import joblib
import numpy as np
from pydantic import BaseModel
import time
import traceback
from typing import Dict, Any
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Orchid Isolation Forest - FULL")

from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
iso_model = None
scaler = None
model_loaded = False
feature_names = [
    'request_length', 'param_count', 'special_char_ratio',
    'url_depth', 'user_agent_length', 'content_length',
    'request_time_seconds', 'status_code',
    'sql_keywords_count', 'html_tag_count', 'path_traversal_count',
    'entropy', 'max_token_length', 'has_equals', 'has_quotes',
    'digit_count', 'letter_count', 'letter_digit_ratio'
]

def load_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
    global iso_model, scaler, model_loaded

    try:
        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Isolation Forest –º–æ–¥–µ–ª–∏...")
        iso_model = joblib.load('models/isolation_forest_real.joblib')
        logger.info("‚úÖ Isolation Forest –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Scaler...")
        scaler = joblib.load('models/scaler.joblib')
        logger.info("‚úÖ Scaler –∑–∞–≥—Ä—É–∂–µ–Ω")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—á–∞—è
        test_features = np.random.randn(1, len(feature_names))
        test_scaled = scaler.transform(test_features)
        prediction = iso_model.predict(test_scaled)
        logger.info(f"‚úÖ –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–µ–Ω, prediction shape: {prediction.shape}")

        model_loaded = True
        logger.info("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ —Ä–∞–±–æ—Ç–µ")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        logger.error(traceback.format_exc())
        model_loaded = False

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
load_models()

class PredictionRequest(BaseModel):
    features: Dict[str, Any]
    metadata: Dict[str, Any]

@app.get("/health")
async def health():
    return {
        "status": "healthy" if model_loaded else "unhealthy",
        "service": "Isolation Forest (FULL)",
        "timestamp": time.time(),
        "model_loaded": model_loaded,
        "model_type": "Isolation Forest",
        "features_count": len(feature_names),
        "version": "2.0-full"
    }

@app.get("/model-info")
async def model_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    if not model_loaded:
        return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}

    return {
        "model_type": "Isolation Forest",
        "n_estimators": iso_model.n_estimators if hasattr(iso_model, 'n_estimators') else "unknown",
        "contamination": iso_model.contamination if hasattr(iso_model, 'contamination') else "unknown",
        "features": feature_names,
        "loaded": model_loaded
    }

@app.post("/predict")
async def predict(request: PredictionRequest):
    """–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML –º–æ–¥–µ–ª–∏"""

    if not model_loaded:
        return {
            "error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞",
            "service": "Isolation Forest",
            "timestamp": time.time(),
            "model_used": False
        }

    try:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        features_list = []
        missing_features = []

        for feature in feature_names:
            if feature in request.features:
                value = request.features[feature]
                try:
                    features_list.append(float(value))
                except:
                    features_list.append(0.0)  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ
            else:
                # –ï—Å–ª–∏ —Ñ–∏—á–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                missing_features.append(feature)
                # –†–∞–∑—É–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                if feature == 'status_code':
                    features_list.append(200.0)
                elif feature == 'request_length':
                    features_list.append(500.0)
                elif feature == 'param_count':
                    features_list.append(3.0)
                elif feature == 'special_char_ratio':
                    features_list.append(0.05)
                elif feature == 'url_depth':
                    features_list.append(2.0)
                elif feature == 'user_agent_length':
                    features_list.append(120.0)
                elif feature == 'content_length':
                    features_list.append(800.0)
                elif feature == 'request_time_seconds':
                    features_list.append(0.5)
                elif feature in ['sql_keywords_count', 'html_tag_count', 'path_traversal_count']:
                    features_list.append(0.0)
                elif feature == 'entropy':
                    features_list.append(2.5)
                elif feature == 'max_token_length':
                    features_list.append(10.0)
                elif feature in ['has_equals', 'has_quotes']:
                    features_list.append(0.0)
                elif feature in ['digit_count', 'letter_count']:
                    features_list.append(5.0)
                elif feature == 'letter_digit_ratio':
                    features_list.append(1.0)
                else:
                    features_list.append(0.0)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array
        features_array = np.array([features_list])

        # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        logger.info(f"Features prepared: {features_list}")

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º
        features_scaled = scaler.transform(features_array)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ Isolation Forest
        prediction = iso_model.predict(features_scaled)[0]  # 1 = –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π, -1 = –∞–Ω–æ–º–∞–ª–∏—è
        anomaly_score = float(iso_model.score_samples(features_scaled)[0])

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–Ω–æ–º–∞–ª–∏—é
        is_anomaly = bool(prediction == -1)

        # –†–∞—Å—á—ë—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if is_anomaly:
            # –î–ª—è –∞–Ω–æ–º–∞–ª–∏–π: —á–µ–º –º–µ–Ω—å—à–µ anomaly_score, —Ç–µ–º –≤—ã—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–æ–±—ã—á–Ω–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π)
            confidence = min(0.99, max(0.5, 1.0 - abs(anomaly_score) / 10))
        else:
            # –î–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö: –æ–±—ã—á–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π, —Ç–æ–∂–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º
            confidence = min(0.99, max(0.5, 1.0 - abs(anomaly_score) / 10))

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞—Ç–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ñ–∏—á–µ–π
        attack_type = "normal"
        if is_anomaly:
            special_char = request.features.get('special_char_ratio', 0)
            param_count = request.features.get('param_count', 0)
            url_depth = request.features.get('url_depth', 0)
            request_len = request.features.get('request_length', 0)
            status_code = request.features.get('status_code', 200)
            sql_kw = request.features.get('sql_keywords_count', 0)
            html_tags = request.features.get('html_tag_count', 0)
            path_trav = request.features.get('path_traversal_count', 0)
            has_quotes = request.features.get('has_quotes', 0)
            has_equals = request.features.get('has_equals', 0)
            entropy = request.features.get('entropy', 0)
            max_token_len = request.features.get('max_token_length', 0)
            digit_count = request.features.get('digit_count', 0)
            letter_count = request.features.get('letter_count', 0)
            letter_digit_ratio = request.features.get('letter_digit_ratio', 1.0)
            # –î–æ–±–∞–≤–∏–º payload –∏ endpoint –∏–∑ metadata –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª
            payload = str(request.metadata.get('payload', ''))
            endpoint = str(request.metadata.get('endpoint', ''))

            if sql_kw > 2 or (special_char > 0.2 and ("'" in payload or '"' in payload) and has_quotes):
                attack_type = "sqli"
            elif html_tags > 0 or (special_char > 0.25 and "<" in payload and ">" in payload):
                attack_type = "xss"
            elif path_trav > 0 or url_depth > 8 or ".." in payload or ".." in endpoint:
                attack_type = "lfi"
            elif request_len > 1500 or param_count > 12 or ";" in payload or "|" in payload:
                attack_type = "rce"
            elif status_code >= 400:
                attack_type = "error_based"
            elif entropy > 4.5 or (max_token_len > 50 and digit_count > 10):
                attack_type = "obfuscated"
            else:
                attack_type = "unknown_anomaly"

        response = {
            "is_anomaly": is_anomaly,
            "anomaly_score": anomaly_score,
            "prediction": attack_type,
            "confidence": round(confidence, 3),
            "service": "Isolation Forest",
            "timestamp": time.time(),
            "model_used": True,
            "features_processed": len(features_list),
            "missing_features": missing_features if missing_features else None,
            "raw_prediction": int(prediction)
        }

        logger.info(f"Prediction result: {response}")
        return response

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
        logger.error(traceback.format_exc())
        return {
            "error": f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}",
            "service": "Isolation Forest",
            "timestamp": time.time(),
            "model_used": False
        }

@app.post("/batch-predict")
async def batch_predict(requests: list):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    if not model_loaded:
        return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}

    results = []
    for req in requests:
        try:
            result = await predict(req)
            results.append(result)
        except Exception as e:
            results.append({"error": str(e)})

    return {
        "results": results,
        "total": len(results),
        "anomalies": sum(1 for r in results if r.get('is_anomaly', False))
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
